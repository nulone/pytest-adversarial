{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest-adversarial Demo\n",
    "\n",
    "This notebook demonstrates how to use pytest-adversarial to find edge cases in your Python code using LLM-powered adversarial testing.\n",
    "\n",
    "## What it does\n",
    "\n",
    "```\n",
    "Your code → LLM generates attacks → Tests that break your code → Export as pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install pytest-adversarial and set up your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "!pip install pytest-adversarial -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API key (OpenRouter or OpenAI)\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Choose one:\n",
    "api_key = getpass(\"Enter your OpenRouter API key: \")\n",
    "os.environ['OPENROUTER_API_KEY'] = api_key\n",
    "\n",
    "# Or for OpenAI:\n",
    "# api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "# os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "print(\"✓ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example Target Code\n",
    "\n",
    "Let's create some example functions to test. These have potential edge cases that the adversarial tester will try to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile target.py\n",
    "import json\n",
    "import re\n",
    "\n",
    "def parse_json(text: str) -> dict:\n",
    "    \"\"\"Parse JSON string to dictionary.\"\"\"\n",
    "    return json.loads(text)\n",
    "\n",
    "def validate_email(email: str) -> bool:\n",
    "    \"\"\"Validate email address format.\"\"\"\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, email))\n",
    "\n",
    "def divide_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "def merge_dicts(dict1: dict, dict2: dict) -> dict:\n",
    "    \"\"\"Merge two dictionaries.\"\"\"\n",
    "    result = dict1.copy()\n",
    "    result.update(dict2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Adversarial Testing\n",
    "\n",
    "Now let's run pytest-adversarial to find edge cases. This will:\n",
    "1. Extract functions from target.py\n",
    "2. Generate adversarial test cases\n",
    "3. Run tests to confirm issues\n",
    "4. Report findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the adversarial tester\n",
    "!pytest-adversarial --target target.py --mode quick --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Tests\n",
    "\n",
    "The successful attacks are automatically saved to `tests/test_adversarial.py`. Let's view them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated tests\n",
    "!cat tests/test_adversarial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try Your Own Code\n",
    "\n",
    "Now it's your turn! Replace the code below with your own function and run adversarial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my_code.py\n",
    "\n",
    "def your_function(input_data):\n",
    "    \"\"\"Replace this with your own function to test.\"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adversarial testing on your code\n",
    "!pytest-adversarial --target my_code.py --mode quick --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Options\n",
    "\n",
    "### Testing Modes\n",
    "\n",
    "| Mode | Rounds | Attacks | Est. Cost |\n",
    "|------|--------|---------|----------|\n",
    "| Quick | 5 | 3/round | ~$0.05 |\n",
    "| Standard | 10 | 5/round | ~$0.15 |\n",
    "| Thorough | 15 | 8/round | ~$0.40 |\n",
    "| Premium (GPT-4o) | 10 | 5/round | ~$1.50 |\n",
    "| Maximum | 20 | 10/round | ~$5.00 |\n",
    "\n",
    "### Get Fix Suggestions\n",
    "\n",
    "Add `--suggest-fixes` to get LLM-generated fix suggestions for found issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest-adversarial --target target.py --mode quick --suggest-fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "- [GitHub Repository](https://github.com/nulone/pytest-adversarial)\n",
    "- [PyPI Package](https://pypi.org/project/pytest-adversarial/)\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Works best on pure functions\n",
    "- LLM-generated tests may have false positives\n",
    "- Not a replacement for professional security audit\n",
    "- Requires API costs per run (~$0.05-5.00 depending on mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
